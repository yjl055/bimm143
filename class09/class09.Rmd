---
title: "Class 09: Unsupervised Learning Mini-Project"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory data analysis

# Preparing the data

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data)
head(wisc.df)
```

```{r}
wisc.data <- as.matrix(wisc.df[,3:32])
```

```{r}
row.names(wisc.data) <- wisc.df$id
```

```{r}
head(wisc.data)
```

```{r}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")

```

<div class="q_box">
-**Q1**. How many observations are in this dataset?
```{r}
dim(wisc.data)
# There are 569 observations in this dataset. 
```
  
-**Q2**. How many variables/features in the data are suffixed with  `_mean`?
```{r}
length(grep("_mean", colnames(wisc.data)))
```

-**Q3**. How many of the observations have a malignant diagnosis?
```{r}
sum (diagnosis)
```
</div>

## Principal Component Analysis

```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Performing PCA
wisc.pr <- prcomp(wisc.data)
summary(wisc.pr, scale=TRUE, center=TRUE)
```

<div class="q_box">
-**Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)? **44.27% is captured by PC1.**

-**Q5**. How many principal components (PCs) are required to describe at least 70% of the original vairance in the data? **At least three PCs are required. At PC3, cumulative proprotion is 72.636%.**

-**Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? **At least seven PCs are required. At PC7, cumulative proportion is 91.010%.**
</div>

## Interpreting PCA results

```{r}
biplot(wisc.pr)
```

<div class="q_box">
-**Q7**. What stands out to you about this plot? Is it easy or difficult to understand? Why? Nothing stands out to me about this plot. It is difficult to understand because rownames are used as the plotting character.
</div>

```{r}
# Scatter plot observations by PC1 and PC2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")
```

```{r}
# Repeat for PC1 and PC3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

<div class="q_box">
-**Q8**. What do you notice about these two plots? **The first plot separates the two subgroups more clearly because PC2 contains more variance in the original data than PC3.**
</div>

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0, 1), type = "o")
```

```{r}
# Plot cumulative proportion of variance explained
plot(pve, xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type = "o")
```

```{r}
## ggplot based graph
install.packages("factoextra")
library(factoextra)
```

```{r}
fviz_eig(wisc.pr, addlabels = TRUE)

```

# Communicating PCA results

<div class="q_box">
-**Q9**. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?
```{r}
wisc.pr$rotation[rownames(wisc.pr$rotation)=="concave.points_mean",]
```


-**Q10**. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
wisc.pr$rotation[rownames(wisc.pr$rotation)=="concave.points_mean",][1]
```
</div>

## Hierarchical clustering

# Hierarchical clustering of case data

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to `data.dist`.
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to `wisc.hclust`.

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

# Results of hierarchical clustering
<div class="q_box">
-**Q11**. Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters? **Around at 20**

```{r}
plot(wisc.hclust)
abline (wisc.hclust, col="red",lty=2)
```
</div>

# Selecting number of clusters

```{r}
# Use `cutree()` to cut the tree so that it has 4 clusters. Assign the output to the variable `wisc.hclust.clusters`.
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
# Creat a table to compare the cluster membership to the actual diagnoses.

table(wisc.hclust.clusters, diagnosis)
```

## K-means clustering

# K-means clustering and comparing results

```{r}
# Create a k-means model on wisc.data: wisc.km
wisc.km <- kmeans(scale(wisc.data), centers=2, nstart=20)

# Compare k-means to actual diagnoses
table(wisc.km$cluster, diagnosis)
```

```{r}
table (wisc.hclust.clusters, diagnosis)
```

## Combining methods

# Clustering on PCA results

```{r}
# Create rgl plot
install.packages("rgl")
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=diagnosis+1)

rglwidget(width = 400, height = 400)
```

